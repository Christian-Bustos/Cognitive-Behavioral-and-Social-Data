# Cognitive-Behavioral-and-Social-Data

## Project Topic:
Comparing Humans, GPT-4, and Claude 2 On Abstraction and Reasoning Tasks: This study investigates the abstract reasoning capabilities of large pre-trained language models (LLMs), specifically GPT-4 and the focus of our experiment Claude 2, using the ConceptARC dataset. This dataset comprises analogy puzzles designed to test a solver’s abstract reasoning skills. Our research reveals a notable performance disparity between these AI models and human solvers, with humansconsistently outperforming the AI systems. This outcome underscores the significance of advanced prompting techniques in enhancing AI performance, indicating that the manner in which problems are presented markedly influences an AI’s conceptual comprehension. Our experimental results suggest that Claude 2 not only exhibits subpar abstraction abilities compared to human levels but also underperforms relative to the abstraction capabilities previously demonstrated by GPT-4 in similar studies.

**Authors:** Davide Christian Mancosu Bustos, Fridoon Najafi, Daniele Pennisi, Karim Eugenio Hamdar

**Email:** mbustosdchristian@gmail.com, davidebustos@libero.it

**Supervisor:** Giuseppe Sartori
